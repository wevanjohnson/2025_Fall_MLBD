---
title: "Cross Validation"
author: | 
  | W. Evan Johnson, Ph.D.
  | Professor, Division of Infectious Disease
  | Director, Center for Data Science
  | Co-Director, Center for Biomedical Informatics and Health AI
  | Rutgers University -- New Jersey Medical School
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{xcolor}
  - \setbeamercolor{frametitle}{fg=black}
  - \usepackage{graphicx}
  - \usebackgroundtemplate{\includegraphics[width=\paperwidth]{cvfigs/RH_template_Page_2.png}}
  - \addtobeamertemplate{frametitle}{\vspace*{.25in}}{\vspace*{.25in}}
  - \setbeamerfont{frametitle}{size=\huge}
  - \usepackage{tikz}
output: 
  beamer_presentation
classoption: aspectratio=169 
editor_options: 
  chunk_output_type: console
tables: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE) 
library(caret)
library(tidyverse)
library(gam)
img_path <- "cvfigs/"
```

## Cross-validation 
\Large
__Cross-validation__ is one of the most important ideas in machine learning. Here we focus on the conceptual and mathematical aspects. We will describe how to implement cross validation in practice in later examples. 

## K-nearest neighbors (kNN)
To motivate the concept, we will introduce an actual machine learning algorithm: __k-nearest neighbors (kNN)__.
\center
![](cvfigs/KnnClassification.svg.png){width=40%}


## K-nearest neighbors (kNN)
\center
![](cvfigs/knn.png){width=60%}

## K-nearest neighbors (kNN)
\Large
With k-nearest neighbors (kNN) we estimate the conditional probability function: 
$$p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).$$
First we define the distance between each of the observations. Then, for any point $(x_1,x_2)$ for which we want an estimate of $p(x_1, x_2)$, we look for the $k$ nearest points to $(x_1,x_2)$ and then take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the _neighborhood_. This gives us an estimate $\hat{p}(x_1,x_2)$.



## K-nearest neighbors (kNN)
\center
Using _k=1_: \    \   \   \   \   \   \   \   \   \  \    \    and _k_=5:

![](cvfigs/Map1NN.png){width=49%} 
![](cvfigs/Map5NN.png){width=49%} 


## K-nearest neighbors (kNN): Example
Using the following dataset:
\small
```{r mnist-27-data, warning=FALSE, message=FALSE, fig.height = 3, fig.width = 6, out.width="60%", fig.align="center"}
library(dslabs); data("mnist_27")
mnist_27$test %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()
```

## K-nearest neighbors (kNN): Example
\Large
We can use the `knn3` function from the `caret` package as follows, with the number of neighbors equal to $k=5$.:

```{r, eval=TRUE}
library(caret)
knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 5)
```

## K-nearest neighbors (kNN): Example
\Large
Since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.

The `predict` function for `knn` produces a probability for each class. 
\small
```{r}
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```

## K-nearest neighbors (kNN): Example

```{r knn-fit, echo=FALSE, message=FALSE, warning=FALSE, out.width="60%",fig.align="center"}
# We use this function to plot the estimated conditional probabilities
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
  geom_raster(show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5),color="black")
}

p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
  ggtitle("kNN-5 estimate")
library(gridExtra)

grid.arrange(p2, p1, nrow=1)
``` 

## Over-training
**Over-training** or **over-fitting** results in having higher accuracy in the train set compared to the test set:
\small
```{r}
y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall["Accuracy"]

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```


## Over-training 

Over-training is at its worst when we set $k=1$:
\small
```{r}
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[["Accuracy"]]
```

\small
```{r}
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall["Accuracy"]
```

## Over-training 
We can see the over-fitting problem in this figure. 
```{r knn-1-overfit, echo=F, message=FALSE, warning=FALSE, out.width="60%",fig.align="center"}
p1 <- mnist_27$true_p %>% 
  mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
  ggplot() +
  geom_point(data = mnist_27$train, aes(x_1, x_2, color= y),
             pch=21, show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
  ggtitle("Train set")

p2 <- mnist_27$true_p %>% 
  mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
  ggplot() +
  geom_point(data = mnist_27$test, aes(x_1, x_2, color= y), 
             pch=21, show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
  ggtitle("Test set")

grid.arrange(p1, p2, nrow=1)
``` 

## Over-smoothing

Although not as badly as with $k=1$, we saw that $k=5$ is also over-trained. Hence, we should consider a larger $k$. Let's try, as an example, a much larger number: $k=401$. 

\small
```{r}
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, 
                         type = "class")
confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall["Accuracy"]
```

## Over-smoothing
```{r mnist-27-glm-est, echo=FALSE, fig.height = 4, fig.width = 6, out.width="70%",fig.align="center"}

p1 <- plot_cond_prob(predict(knn_fit_1, mnist_27$true_p)[,2]) +
  ggtitle("kNN-1")

p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
  ggtitle("kNN-5")

p3 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) +
  ggtitle("kNN-401")
  
grid.arrange(p1, p2, p3, nrow=1)
```



## Picking the $k$ in kNN
\Large
So how do we pick $k$? In principle, we want to pick the $k$ that maximizes accuracy, or minimizes the expected MSE (defined later). 

The goal of **cross validation** is to estimate these quantities for any given algorithm and set of tuning parameters such as $k$. To understand why we need a special method to do this let's repeat what we did above but for different values of $k$:

```{r}
ks <- seq(3, 251, 2)
```

## Picking the $k$ in kNN
We do this using  `map_df` function to repeat the above for each one. 
\small
```{r, warning=FALSE, message=FALSE}
library(purrr)
accuracy <- map_df(ks, function(k){
  fit <- knn3(y ~ ., data = mnist_27$train, k = k)
  
  y_hat <- predict(fit, mnist_27$train, type = "class")
  cm_train <- confusionMatrix(y_hat, mnist_27$train$y)
  train_error <- cm_train$overall["Accuracy"]
  
  y_hat <- predict(fit, mnist_27$test, type = "class")
  cm_test <- confusionMatrix(y_hat, mnist_27$test$y)
  test_error <- cm_test$overall["Accuracy"]
  
  tibble(train = train_error, test = test_error)
})
```

## Picking the $k$ in kNN
\Large
We can plot the accuracy estimates for each value of $k$:

```{r accuracy-vs-k-knn, echo=FALSE, fig.height = 3, fig.width = 6, out.width="65%",fig.align="center"}
accuracy %>% mutate(k = ks) %>%
  gather(set, accuracy, -k) %>%
  mutate(set = factor(set, levels = c("train", "test"))) %>%
  ggplot(aes(k, accuracy, color = set)) + 
  geom_line() +
  geom_point() 
```

## Picking the $k$ in kNN
\Large
If we were to use these estimates to pick the $k$ that maximizes accuracy, we would use the estimates built on the test data:

```{r}
ks[which.max(accuracy$test)]
max(accuracy$test)
```

## Mathematical description of cross validation
\Large
A common goal of machine learning is to find an algorithm that produces predictors $\hat{Y}$ for an outcome $Y$ that minimizes the MSE:

$$
\mbox{MSE} = \mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
$$

## Mathematical description of cross validation
\Large
When all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this:

$$
\hat{\mbox{MSE}} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$
These two are often referred to as the _true error_ and _apparent error_, respectively.

## Mathematical description of cross validation
\Large
There are two important characteristics we should keep in mind:

1. Our data is random so the apparent error is a random variable. One algorithm may have a lower apparent error than another algorithm due to luck.

2. If we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining.  We will see an extreme example of this with k-nearest neighbors.

## Mathematical description of cross validation
\Large
**Cross validation** is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to $B$ new random samples of the data, none of them used to train the algorithm:
$$\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 $$
with $B$ a large number that can be thought of as practically infinite. 

## Mathematical description of cross validation
\Large
This is a theoretical quantity because we only have available one set of outcomes: $y_1, \dots, y_n$. Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have. To do this, we have to generate a series of different random samples. 

<!--
```{r, include=FALSE}
if(knitr::is_html_output()){
  knitr::opts_chunk$set(out.width = "500px",
                        out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"')
} else{
  knitr::opts_chunk$set(out.width = "35%")
}
```
-->

## K-fold cross validation
\Large
The first one we describe is __K-fold cross validation__. A machine learning challenge starts with a dataset (blue). We need to use this to build an algorithm that will be used in an independent validation dataset (yellow).
\center
![](cvfigs/cv-1.png){height=50%}

## K-fold cross validation
\Large
But we don't get to see these independent datasets. 
\center
![](cvfigs/cv-2.png){height=50%}

## K-fold cross validation
\Large
We create a __training set__ (blue) and a __test set__ (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.
\center
![](cvfigs/cv-3.png){height=50%}

<!--
## K-fold cross validation
We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing.
-->

## K-fold cross validation
\Large
We need to optimize algorithm parameters ($\lambda$) without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain.

For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.

## K-fold cross validation
\Large
Before we start the cross validation, we fix all the algorithm parameter estimates $\hat{y}_i(\lambda)$.

So, if we are going to imitate this definition:
$$\mbox{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 $$

We want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation, we do it $K$ times. We are showing an example that uses $K=5$. 

## K-fold cross validation
\Large First, we simply pick $M=N/K$ observations at random and think of these as a random sample $y_1^b, \dots, y_M^b$, with $b=1$. We call this the validation set:

\center
![](cvfigs/cv-4.png){height=50%}

## K-fold cross validation
\Large
Now we can fit the model in the training set, then compute the apparent error on the independent set:
$$\hat{\mbox{MSE}}_b(\lambda) = \frac{1}{M}\sum_{i=1}^M \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 $$

## K-fold cross validation
In K-fold cross validation, we randomly split the observations into $K$ non-overlapping sets:

\center
![](cvfigs/cv-5.png){height=40%}

## K-fold cross validation
\Large
Now we repeat the calculation above for each of these sets $b=1,\dots,K$ and obtain $\hat{\mbox{MSE}}_1(\lambda),\dots, \hat{\mbox{MSE}}_K(\lambda)$. Then, for our final estimate, we compute the average:

$$
\hat{\mbox{MSE}}(\lambda) = \frac{1}{B} \sum_{b=1}^K \hat{\mbox{MSE}}_b(\lambda)
$$

and obtain an estimate of our loss. A final step would be to select the $\lambda$ that minimizes the MSE.

## K-fold cross validation
We have to take into account the fact that the optimization occurred on the training data:

\center
![](cvfigs/cv-6.png){height=50%}

## K-fold cross validation
We can do cross validation again:

\center
![](cvfigs/cv-7.png){height=65%}

## K-fold cross validation

\center
![](cvfigs/cv-8.png){height=65%}

## K-fold cross validation
\Large 
Now how do we pick the cross validation $K$? Large values of $K$ are preferable because the training data better imitates the original dataset. However, larger values of $K$ will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of $K=3$, $K=4$, $K=5$ and $K=10$ are popular.

## K-fold cross validation
\Large
One way we can improve the variance of our final estimate is to take more samples-- we can just pick $K$ sets of some size at random.

One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the _bootstrap_. In fact, this is the default approach in the __caret__ package.  In the next section, we include an explanation of how the bootstrap works in general.

## Bootstrap

Suppose the income distribution of your population is as follows:

```{r income-distribution,  fig.align="center"}
set.seed(1995)
n <- 10^6
income <- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I("black"))
```

## Bootstrap
\Large
The population median is: 

```{r}
m <- median(income)
m
```

## Bootstrap
\Large
Suppose we don't have access to the entire population, but want to estimate the median $m$. We take a sample of 100 and estimate the population median $m$ with the sample median $M$:

```{r}
N <- 100
X <- sample(income, N)
median(X)
```

How to construct a CI? What is the distribution of $M$?

## Bootstrap
We can use a Monte Carlo simulation to learn the distribution of $M$.
\small
```{r median-is-normal, message=FALSE, warning=FALSE, out.width="30%", fig.width = 6, fig.height = 3, fig.align="center"}
library(gridExtra)
B <- 10^4
M <- replicate(B, {X <- sample(income, N); median(X)})
p1 <- qplot(M, bins = 30, color = I("black"))
p2 <- qplot(sample = scale(M), xlab = "theoretical", 
            ylab = "sample") + geom_abline()
grid.arrange(p1, p2, ncol = 2)
```

## Bootstrap
\Large
If we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. We can see that the 95% confidence interval based on CLT

```{r}
median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)
```

## Bootstrap
\Large
This is quite different from the confidence interval we would generate if we know the actual distribution of $M$:

```{r}
quantile(M, c(0.025, 0.975))
```

## Bootstrap
This is how we construct bootstrap samples and an approximate distribution:

```{r}
B <- 10^4
M_star <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  median(X_star)
})
```

Note a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution: 

```{r}
quantile(M_star, c(0.025, 0.975))
```

## Bootstrap
\Large
For more on the Bootstrap, including corrections one can apply to improve these confidence intervals, please consult the book _An introduction to the bootstrap_ by Efron, B., & Tibshirani, R. J.


*Note that we can use ideas similar to those used in the bootstrap in cross validation: instead of dividing the data into equal partitions, we simply bootstrap many times.*


## The caret package {#caret}
\Large
The __caret__ package tries to consolidate these differences and provide consistency. It currently includes 237 different methods which are summarized in the __caret__ package manual^[https://topepo.github.io/caret/available-models.html]. 
\vskip .2in

Keep in mind that __caret__ does not include the needed packages and, to implement a package through __caret__, you still need to install the library. The required packages for each method are described in the package manual.

## The caret `train` functon 
\Large
The __caret__ package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this incredibly helpful package. We will use the 2 or 7 example to illustrate:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
data("mnist_27")
```

## The caret `train` functon 
\Large
The __caret__ `train` function lets us train different algorithms using similar syntax. So, for example, we can type:

```{r}
library(caret)
train_glm <- train(y ~ ., method = "glm", 
                   data = mnist_27$train)
train_knn <- train(y ~ ., method = "knn",
                   data = mnist_27$train)
```

## The caret `train` functon 
\Large
To make predictions, we can use the output of this function directly without needing to look at the specifics of `predict.glm` and `predict.knn`. Instead, we can learn how to obtain predictions from `predict.train`.

The code looks the same for both methods:
```{r}
y_hat_glm <- predict(train_glm, 
                     mnist_27$test, type = "raw")
y_hat_knn <- predict(train_knn, 
                     mnist_27$test, type = "raw")
```

## The caret `train` functon 
\Large
This permits us to quickly compare the algorithms:
```{r}
confusionMatrix(y_hat_glm, 
      mnist_27$test$y)$overall[["Accuracy"]]
confusionMatrix(y_hat_knn,
      mnist_27$test$y)$overall[["Accuracy"]]
```

## Cross validation {#caret-cv}
\Large
When an algorithm includes a tuning parameter, `train` automatically uses cross validation to decide among a few default values. To find out what parameter or parameters are optimized, you can read the manual ^[http://topepo.github.io/caret/available-models.html] or study the output of: 

```{r, eval=FALSE}
getModelInfo("knn")
```

## Cross validation {#caret-cv}
\Large
We can also use a quick lookup like this:

```{r, eval=FALSE}
modelLookup("knn")
```

If we run it with default values: 
```{r}
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train)
```

## Cross validation 
You can quickly see the results of the cross validation using the `ggplot` function. The argument `highlight` highlights the max:

```{r caret-highlight, fig.height = 3, fig.width = 6, out.width="60%",fig.align="center"}
ggplot(train_knn, highlight = TRUE)
```

## Cross validation 
\Large
By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. For the `kNN` method, the default is to try $k=5,7,9$. We change this using the `tuneGrid` parameter. The grid of values must be supplied by a data frame with the parameter names as specified in the `modelLookup` output. 

## Cross validation 
\Large
Here, we present an example where we try out 30 values between 9 and 67. To do this with __caret__, we need to define a column named `k`, so we use this: 
`data.frame(k = seq(9, 67, 2))`.

Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples. Since we are fitting $30 \times 25 = 750$ kNN models. 

## Cross validation 
\Large
We will set the seed because cross validation is a random procedure and we want to make sure the result here is reproducible.

```{r eval=F}
set.seed(2008)
train_knn <- train(y ~ ., method = "knn", 
      data = mnist_27$train,
      tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```

## Cross validation 
```{r train-knn-plot1, echo=F, fig.height = 3, fig.width = 6, out.width="90%",fig.align="center"}
set.seed(2008)
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```


## Cross validation 
\Large
To access the parameter that maximized the accuracy, you can use this:

```{r}
train_knn$bestTune
```

## Cross validation 
\Large
and the best performing model like this:

```{r}
train_knn$finalModel
```

## Cross validation 
\Large
The function `predict` will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:
```{r}
confusionMatrix(predict(train_knn, 
                        mnist_27$test, type = "raw"),
                mnist_27$test$y)$overall["Accuracy"]
```

## Cross validation 
\Large
If we want to change how we perform cross validation, we can use the `trainControl` function. We can make the code above go a bit faster by using, for example, 10-fold cross validation:
```{r cv-10-fold-accuracy-estimate, eval=F, fig.height = 3, fig.width = 6, out.width="60%",fig.align="center"}
control <- trainControl(method = "cv", 
               number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn", 
              data = mnist_27$train,
              tuneGrid = data.frame(k = seq(9, 71, 2)),
              trControl = control)
ggplot(train_knn_cv, highlight = TRUE)
```

## Cross validation 
```{r cv-10-fold-accuracy-estimate1, echo=F, fig.height = 3, fig.width = 6, out.width="90%",fig.align="center"}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)),
                   trControl = control)
ggplot(train_knn_cv, highlight = TRUE)
```

<!--
## Cross validation 
We notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.

Note that `results` component of the `train` output includes several summary statistics related to the variability of the cross validation estimates:

```{r}
names(train_knn$results)
```
--> 

## Cross validation 
\Large
We can also see the standard deviation bars obtained from the cross validation samples:

```{r accuracy-with-sd-bars, eval=F, fig.height = 3, fig.width = 6, out.width="70%",fig.align="center"}
train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() + geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD, 
                    ymax = Accuracy + AccuracySD))
```


## Cross validation 
```{r accuracy-with-sd-bars1, echo=F, fig.height = 3, fig.width = 6, out.width="90%",fig.align="center"}
train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() + geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD, 
                    ymax = Accuracy + AccuracySD))
```

## Example: fitting with loess

```{r, echo=FALSE, out.width="70%",fig.align="center"}
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
  geom_raster(show.legend = FALSE) +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5),color="black")
}
```

The best fitting kNN model approximates the true conditional probability:
```{r mnist27-optimal-knn-fit, echo=FALSE, out.width="60%",fig.align="center"}
plot_cond_prob(predict(train_knn, mnist_27$true_p, type = "prob")[,2])
```

## Example: fitting with loess
\Large
However, we do see that the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models part of the manual^[https://topepo.github.io/caret/available-models.html] we see that we can use the `gamLoess` method. 
In the manual^[https://topepo.github.io/caret/train-models-by-tag.html] we also see that we need to install the __gam__ package if we have not done so already:

```{r, eval=FALSE}
install.packages("gam")
```

## Example: fitting with loess
Then we see that we have two parameters to optimize:

```{r}
modelLookup("gamLoess")
```

We will stick to a degree of 1. But to try out different values for the span, we still have to include a column in the table with the name `degree` so we can do this:

```{r}
grid <- expand.grid(span = seq(0.15, 0.65, len = 10)
                    , degree = 1)
```

## Example: fitting with loess
\Large
We will use the default cross validation control parameters.
```{r loess-accuracy, eval=F, warning=FALSE, message=FALSE, out.width="60%",fig.align="center"}
train_loess <- train(y ~ ., method = "gamLoess", 
                   tuneGrid=grid,
                   data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)
```


## Example: fitting with loess
```{r loess-accuracy1, echo=F, warning=FALSE, message=FALSE, out.width="60%",fig.align="center"}
train_loess <- train(y ~ ., method = "gamLoess", 
                   tuneGrid=grid,
                   data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)
```

## Example: fitting with loess
\Large
We can see that the method performs similar to kNN:
```{r}
confusionMatrix(data = predict(
      train_loess, mnist_27$test), 
      reference = mnist_27$test$y)$overall["Accuracy"]
```

## Example: fitting with loess
It produces a smoother estimate of the conditional probability:

```{r gam-smooth, warning=FALSE, echo=FALSE, out.width="60%",fig.align="center"}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(train_loess, mnist_27$true_p, type = "prob")[,2]) + 
  ggtitle("GAM Loess estimate")

gridExtra::grid.arrange(p2, p1, nrow=1)
```


## Session Info
\tiny
```{r session}
sessionInfo()
```
